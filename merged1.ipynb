{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 23: Final tool\n",
    "\n",
    "*The final tool for the process mining project.*\n",
    "\n",
    "This tool contains five models:\n",
    "1. Naive predictor: Type\n",
    "2. Naive predictor: Time\n",
    "3. Random Forest: Type\n",
    "4. Neural Network: Time\n",
    "5. Neural Network: Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import *\n",
    "import time\n",
    "# next command ensures that plots appear inside the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # also improves the look of plots\n",
    "import scipy\n",
    "sns.set()\n",
    "plt.rcParams['figure.figsize'] = 10, 5  # default hor./vert. size of plots, in inches\n",
    "plt.rcParams['lines.markeredgewidth'] = 1  # to fix issue with seaborn box plots; needed after import seaborn\n",
    "\n",
    "# Init Pandas settings\n",
    "pd.set_option(\"mode.chained_assignment\", None) # to remove false positive chained assignment warnings\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Start the timer to measure how long the notebook takes\n",
    "start1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"BPI_Challenge_2012.xes.gz_UNPACKED.csv\", index_col=[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['org:resource'], inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df['datetime'] = pd.to_datetime(df['time:timestamp'], errors='coerce', utc=True)\n",
    "df['next_event'] = df['concept:name'].shift(-1)\n",
    "df['next_case'] = df['case:concept:name'].shift(-1).fillna(0)\n",
    "df['next_datetime'] = df['datetime'].shift(-1)\n",
    "df['timedelta'] = (df['next_datetime'] - df['datetime']).astype('timedelta64[s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:10]\n",
    "trainlen = int(len(df)*0.7)\n",
    "dftrain = df[:trainlen]\n",
    "dftest = df[trainlen:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Naive baseline\n",
    "## 2.1 Type prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most common next event type for the given ev_type\n",
    "# This function replaces block 10 til 15 (10: \"def nth_most_common(w, n):\", 15: \"next_common_train\")\n",
    "def get_most_common_next_type(n, ev_type):\n",
    "    # Find all rows of the given event type\n",
    "    target_rows = n[n['concept:name'] == ev_type]\n",
    "    successors = []\n",
    "    \n",
    "    # Loop over all rows of the given event type\n",
    "    for idx, row in target_rows.iterrows():\n",
    "        \n",
    "        # Only check the next row if it exists in the DF\n",
    "        if idx + 1 < len(n):\n",
    "            current_case = row['case:concept:name']\n",
    "            next_case = n.loc[n.index[idx + 1], 'case:concept:name']\n",
    "\n",
    "            # If the next event in the DF is in the same case, add the event type to the successors list\n",
    "            if current_case == next_case:\n",
    "                successors.append(n.loc[n.index[idx + 1], 'concept:name'])\n",
    "    \n",
    "    # Return the most common event type in the successors list\n",
    "    return max(set(successors), key=successors.count) if len(successors) > 0 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_types = df['concept:name'].unique()\n",
    "\n",
    "most_common_next_types = {}\n",
    "for ev_type in event_types:\n",
    "    most_common_next_types[ev_type] = get_most_common_next_type(dftrain, ev_type=ev_type)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame(dftrain[['next_case', 'case:concept:name', 'concept:name', 'timedelta' ]])\n",
    "\n",
    "time_till_next = {}\n",
    "\n",
    "for ev_type in event_types:\n",
    "    target_rows = dfs[dfs['concept:name'] == ev_type]\n",
    "\n",
    "    a = 0\n",
    "    c = 0\n",
    "    for idx, row in target_rows.iterrows():\n",
    "        if row['case:concept:name'] == row['next_case']:\n",
    "            a += row['timedelta']\n",
    "            c += 1\n",
    "    \n",
    "    #print(ev_type, a, c)\n",
    "    time_till_next[ev_type] = a/c\n",
    "\n",
    "time_till_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Compiling baseline into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest['timedelta_baseline'] = dftest['concept:name'].map(time_till_next)\n",
    "dftest['next_event_baseline'] = dftest['concept:name'].map(most_common_next_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest: Type prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rft = pd.read_csv('https://raw.githubusercontent.com/NickSot/process_mining/main/merged_files/bpi_2012_rft.csv')\n",
    "data_rft.drop('Unnamed: 0.1', axis=1, inplace=True)\n",
    "data_rft['next_case'] = data_rft['next_case'].fillna(0).astype(int)\n",
    "data_rft['lifecycle + event'] = data_rft['lifecycle:transition'] + ' ' + data_rft['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rft.rename(columns={'A_SUBMITTED': 'A_SUBMITTED_',\n",
    "                      'A_PARTLYSUBMITTED': 'A_PARTLYSUBMITTED_',\n",
    "                      'A_PREACCEPTED': 'A_PREACCEPTED_',\n",
    "                      'W_Completeren aanvraag': 'W_Completeren aanvraag_',\n",
    "                      'A_ACCEPTED' : 'A_ACCEPTED_', \n",
    "                      'O_SELECTED': 'O_SELECTED_', \n",
    "                      'A_FINALIZED': 'A_FINALIZED_',\n",
    "                      'O_CREATED': 'O_CREATED_', \n",
    "                      'O_SENT': 'O_SENT_', \n",
    "                      'W_Nabellen offertes': 'W_Nabellen offertes_', \n",
    "                      'O_SENT_BACK': 'O_SENT_BACK_',\n",
    "                      'W_Valideren aanvraag': 'W_Valideren aanvraag_', \n",
    "                      'A_REGISTERED': 'A_REGISTERED_', \n",
    "                      'A_APPROVED': 'A_APPROVED_', \n",
    "                      'O_ACCEPTED': 'O_ACCEPTED_',\n",
    "                      'A_ACTIVATED': 'A_ACTIVATED_', \n",
    "                      'O_CANCELLED': 'O_CANCELLED_', \n",
    "                      'A_DECLINED': 'A_DECLINED_', \n",
    "                      'A_CANCELLED': 'A_CANCELLED_',\n",
    "                      'W_Afhandelen leads': 'W_Afhandelen leads_',\n",
    "                      'W_Wijzigen contractgegevens': 'W_Wijzigen contractgegevens_',\n",
    "                      'W_Beoordelen fraude': 'W_Beoordelen fraude_',\n",
    "                      'O_DECLINED': 'O_DECLINED_', \n",
    "                      'W_Nabellen incomplete dossiers': 'W_Nabellen incomplete dossiers_',\n",
    "                      'W_Beoordelen fraude': 'W_Beoordelen fraude'}, \n",
    "             inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_rft[:(int((len(data_rft)*0.7)))]\n",
    "test = data_rft[(int((len(data_rft)*0.7))):]\n",
    "all = data_rft\n",
    "train, test, all = train.dropna(), test.dropna(), all.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dummy variables from the event, in this way the model can use all the different event types.\n",
    "event_train = pd.get_dummies(train['event'])\n",
    "event_test = pd.get_dummies(test['event'])\n",
    "\n",
    "lifecycle_train_ = pd.get_dummies(train['lifecycle:transition'])\n",
    "lifecycle_test_ = pd.get_dummies(test['lifecycle:transition'])\n",
    "\n",
    "lifecycle_train = pd.get_dummies(train['lifecycle + event'])\n",
    "lifecycle_test = pd.get_dummies(test['lifecycle + event'])\n",
    "\n",
    "lifecycle_train_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([train, event_train], axis=1)\n",
    "df_test = pd.concat([test, event_test], axis=1)\n",
    "\n",
    "df_train_1 = pd.concat([df_train, lifecycle_train_], axis=1)\n",
    "df_test_1 = pd.concat([df_test, lifecycle_test_], axis=1)\n",
    "\n",
    "df_train_2 = pd.concat([df_train_1, lifecycle_train], axis=1)\n",
    "df_test_2 = pd.concat([df_test_1, lifecycle_test], axis=1)\n",
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the rows the model doesn't need.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_ = df_train_2.drop(columns=['Unnamed: 0', 'lifecycle:transition', 'lifecycle + event', 'event', 'time:timestamp', 'case:REG_DATE', 'next_event', 'W_Valideren aanvraag', 'W_Wijzigen contractgegevens_', 'COMPLETE O_DECLINED', 'START', 'next_case'])\n",
    "y_train = df_train_2['next_event']\n",
    "\n",
    "X_test = df_test_2.drop(columns=['Unnamed: 0', 'lifecycle:transition', 'lifecycle + event', 'event', 'time:timestamp', 'case:REG_DATE', 'next_event', 'W_Valideren aanvraag', 'W_Wijzigen contractgegevens_', 'COMPLETE O_DECLINED', 'START',  'next_case'])\n",
    "y_test = df_test_2['next_event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling all features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train_)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training the RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['next_event_rfc_pred'] = rfc_pred\n",
    "dff = pd.concat([dftest, test[['next_event_rfc_pred']]], axis=1)\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Visualizing model performance\n",
    "### 3.3.1 Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "labels = list(np.unique(y_test))\n",
    "cm =confusion_matrix(y_test, rfc_pred, labels=labels)\n",
    "\n",
    "# Normalise\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(20,15))\n",
    "sns.heatmap(cmn, annot=True, fmt='.2f', cmap='Blues', ax=ax)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(np.unique(y_test))\n",
    "a =  confusion_matrix(y_test, rfc_pred, labels=labels)\n",
    "\n",
    "cmd = pd.DataFrame(a, index=labels, columns=labels)\n",
    "\n",
    "cmdn = cmd.div(cmd.sum(axis=1), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,17)) \n",
    "sns.heatmap(cmdn, annot=True, cmap='Blues', ax=ax)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('actual')\n",
    "fig.savefig('norm_confusion_matrix.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test, rfc_pred, output_dict = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the feature importance for the model with name \"rfr\"\n",
    "importances = rfc.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rfc.estimators_], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a bar chart\n",
    "X_train_col = X_train_.columns\n",
    "forest_importances = pd.Series(importances)\n",
    "df_importances = pd.concat([forest_importances, pd.Series(X_train_col)], axis=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "df_importances.plot.bar(yerr=std, ax=ax, x=1, y=0, figsize=(18, 10))\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Neural Network: Time prediction\n",
    "\n",
    "For the time prediction, two neural networks have been constructed. The first makes a rough estimate, after which the second is used to predict the precise values.\n",
    "\n",
    "This approach was chosen because the original single network implementation performed very bad on smaller values. This approach allows us to handle the estimated lower values separately and make more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "np.random.seed(1)\n",
    "df = pd.read_csv('BPI_Challenge_2012.xes.gz_UNPACKED.csv', index_col=[0])\n",
    "start = time.time()\n",
    "df.drop(columns=['org:resource'], inplace=True)\n",
    "df['next_event'] = df['concept:name'].shift(-1)\n",
    "df['previous_event'] = df['concept:name'].shift(1)\n",
    "df['pp_event'] = df['concept:name'].shift(2)\n",
    "df['ppp_event'] = df['concept:name'].shift(3)\n",
    "df['p_lifecycle:transition'] = df['lifecycle:transition'].shift(1)\n",
    "df['next_case'] = df['case:concept:name'].shift(-1).fillna(0)\n",
    "df['next_case1'] = df['next_case'].shift(-1).fillna(0)\n",
    "df['datetime'] = pd.to_datetime(df['time:timestamp'], errors='coerce', utc=True)\n",
    "df['weekday'] = df['datetime'].dt.weekday\n",
    "df['previous_weekday'] = df['weekday'].shift(1)\n",
    "df['hour'] =  df['datetime'].dt.hour\n",
    "df['minute'] =  df['datetime'].dt.minute\n",
    "df['month'] =  df['datetime'].dt.month\n",
    "df['next_datetime'] = df['datetime'].shift(-1)\n",
    "df['timedelta'] = (df['next_datetime'] - df['datetime']).astype('timedelta64[s]')\n",
    "df = df[(df['case:concept:name'].astype(int)) == (df['next_case'].astype(int))]\n",
    "df['previous_timedelta'] = df['timedelta'].shift(1)\n",
    "df['pp_timedelta'] = df['timedelta'].shift(2)\n",
    "df['ppp_timedelta'] = df['timedelta'].shift(3)\n",
    "print(len(df))\n",
    "df.dropna(inplace=True)\n",
    "df.iloc[:10]\n",
    "dfxx = df[['hour', 'minute', 'previous_timedelta', 'pp_timedelta', 'ppp_timedelta']]\n",
    "dfx = df[['concept:name', 'previous_event', 'lifecycle:transition', 'pp_event', 'p_lifecycle:transition', 'weekday']]\n",
    "dfy = (df[['timedelta']])\n",
    "valx = dfx\n",
    "valy = dfy\n",
    "dummies = pd.get_dummies(dfx.astype('str'))\n",
    "dfx = dfxx.join(dummies)\n",
    "trainlen = int(len(df)*0.7)\n",
    "x_train, y_train = dfx[:trainlen], dfy[:trainlen]\n",
    "x_test, y_test = dfx[trainlen:], dfy[trainlen:]\n",
    "df2 = x_train.join(y_train)\n",
    "dftest = x_test.join(y_test)\n",
    "valy = y_test\n",
    "#print(x_train.iloc[:10])\n",
    "columns = y_train.columns\n",
    "x_train = tf.convert_to_tensor(x_train)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "x_test = tf.convert_to_tensor(x_test)\n",
    "y_test = tf.convert_to_tensor(y_test)\n",
    "print(len(x_test))\n",
    "df.iloc[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Creation of the first estimator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitter NN\n",
    "lrelu = tf.keras.layers.LeakyReLU(alpha=0.3)\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(x_train.shape[1],1)),\n",
    "  tf.keras.layers.BatchNormalization(),  \n",
    "  tf.keras.layers.Dense(228, activation='swish', activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dense(128, activation='swish', activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dense(48, activation='swish', activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "  tf.keras.layers.BatchNormalization(), \n",
    "  tf.keras.layers.Dense(48, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "model.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\n",
    "\n",
    "ftrl = tf.keras.optimizers.Ftrl(\n",
    "    learning_rate=0.1,\n",
    "    learning_rate_power=-0.5,\n",
    "    initial_accumulator_value=0.1,\n",
    "    l1_regularization_strength=0.01,\n",
    "    l2_regularization_strength=0.01,\n",
    "    name=\"Ftrl\",\n",
    "    l2_shrinkage_regularization_strength=0.01,\n",
    "    beta=0.1)\n",
    "\n",
    "msle = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "def loss_comb(y_true, y_pred):\n",
    "    return 1*msle(y_true, y_pred) + 0.00000001*(loss_fn(y_true, y_pred))\n",
    "\n",
    "model.compile(optimizer=ftrl,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Training the estimator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitter NN\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=512, validation_split=0.0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Evaluating the estimator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Displaying predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(x_test).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = model(x_train).numpy()\n",
    "predictions_test = model(x_test).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['predictions_train'] = predictions_train\n",
    "dftest['predictions_test'] = predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test and train split on split-NN prediction\n",
    "p=50\n",
    "df2_short = df2[df2['predictions_train'] < np.nanpercentile(df2['predictions_train'], p)]\n",
    "df2_long = df2[df2['predictions_train'] >= np.nanpercentile(df2['predictions_train'], p)]\n",
    "dftest_long = dftest[dftest['predictions_test'] >= np.nanpercentile(dftest['predictions_test'], p)]\n",
    "dftest_short = dftest[dftest['predictions_test'] < np.nanpercentile(dftest['predictions_test'], p)]\n",
    "print(np.nanpercentile(dftest['predictions_test'], p))\n",
    "print(np.nanpercentile(df2['predictions_train'], p))\n",
    "df2.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates short training data\n",
    "x = df2_short.drop(columns=['timedelta', 'predictions_train'])\n",
    "y = pd.DataFrame(df2_short['timedelta'])\n",
    "x_train, y_train = x, y\n",
    "#print(x_train.iloc[:10])\n",
    "columns = y_train.columns\n",
    "x_train = tf.convert_to_tensor(x_train)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Creation of second neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHORT MODEL \n",
    "model_short = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(x_train.shape[1],1)),\n",
    "  tf.keras.layers.BatchNormalization(),  \n",
    "  tf.keras.layers.Dense(128, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "  tf.keras.layers.Dense(48, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "  tf.keras.layers.Dense(48, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model_short.summary()\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\n",
    "\n",
    "ftrl = tf.keras.optimizers.Ftrl(\n",
    "    learning_rate=0.1,\n",
    "    learning_rate_power=-0.7,\n",
    "    initial_accumulator_value=0.1,\n",
    "    l1_regularization_strength=0.01,\n",
    "    l2_regularization_strength=0.01,\n",
    "    name=\"Ftrl\",\n",
    "    l2_shrinkage_regularization_strength=0.01,\n",
    "    beta=0.1)\n",
    "\n",
    "model_short.compile(optimizer=ftrl,\n",
    "              loss=loss_comb,\n",
    "              metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Training the second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_short.fit(x_train, y_train, epochs=5, batch_size=512, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dftest_short.drop(columns=['timedelta', 'predictions_test'])\n",
    "y = pd.DataFrame(dftest_short['timedelta'])\n",
    "x_train, y_train = x, y\n",
    "#print(x_train.iloc[:10])\n",
    "columns = y_train.columns\n",
    "x_train = tf.convert_to_tensor(x_train)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "predictions_test_short = model_short(x_train).numpy()\n",
    "dftest_short['predictions'] = predictions_test_short\n",
    "dftest_short.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dftest_long.drop(columns=['timedelta', 'predictions_test'])\n",
    "y = pd.DataFrame(dftest_long['timedelta'])\n",
    "x_train, y_train = x, y\n",
    "columns = y_train.columns\n",
    "x_train = tf.convert_to_tensor(x_train)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "predictions_test_long = model(x_train).numpy()\n",
    "dftest_long['predictions'] = predictions_test_long\n",
    "dftest_long.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Evaluating the second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('short old test log', np.square(np.log2(dftest_short['timedelta'] + 1.) - np.log2(dftest_short['predictions_test'] + 1.)).mean())\n",
    "print('short new test log',np.square(np.log2(dftest_short['timedelta'] + 1.) - np.log2(dftest_short['predictions'] + 1.)).mean(), '\\n')\n",
    "print('short old test mse',np.square(abs(dftest_short['timedelta'] - dftest_short['predictions_test'])).mean())\n",
    "print('short new test mse',np.square(abs(dftest_short['timedelta'] - dftest_short['predictions'])).mean(), '\\n')\n",
    "print('long old test mse', np.square(abs(dftest_long['timedelta'] - dftest_long['predictions_test'])).mean())\n",
    "print('long new test mse', np.square(abs(dftest_long['timedelta'] - dftest_long['predictions'])).mean(), '\\n')\n",
    "print('long old test log', np.square(np.log2(dftest_long['timedelta'] + 1.) - np.log2(dftest_long['predictions_test'] + 1.)).mean())\n",
    "print('long new test log', np.square(np.log2(dftest_long['timedelta'] + 1.) - np.log2(dftest_long['predictions'] + 1.)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest_all = pd.concat([dftest_long, dftest_short], sort=False).sort_index()\n",
    "dftest_all['error'] = abs(dftest_all['predictions'] - dftest_all['timedelta'])\n",
    "#dftest_all.drop(columns=['predictions_test'], inplace=True)\n",
    "dftest_all[-10:]\n",
    "#dff[-10:]\n",
    "dfff = pd.concat([dff, dftest_all[['predictions']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffj = dff.join(pd.DataFrame(dftest_all['predictions']))\n",
    "dffj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Neural Network: Type prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('BPI_Challenge_2012.xes.gz_UNPACKED.csv', index_col=[0])\n",
    "start = time.time()\n",
    "df.drop(columns=['org:resource'], inplace=True)\n",
    "df['next_event'] = df['concept:name'].shift(-1)\n",
    "df['next_case'] = df['case:concept:name'].shift(-1)\n",
    "df['previous_event'] = df['concept:name'].shift(1)\n",
    "df['pp_event'] = df['concept:name'].shift(2)\n",
    "df['ppp_event'] = df['concept:name'].shift(3)\n",
    "df['p_lifecycle:transition'] = df['lifecycle:transition'].shift(1)\n",
    "#df = df[df['case:concept:name'] == df['next_case']]\n",
    "df['datetime'] = pd.to_datetime(df['time:timestamp'], errors='coerce', utc=True)\n",
    "df['day'] = df['datetime'].dt.day\n",
    "df['hour'] =  df['datetime'].dt.hour\n",
    "df['minute'] =  df['datetime'].dt.minute\n",
    "df['month'] =  df['datetime'].dt.month\n",
    "dfx = df[['concept:name', 'previous_event', 'lifecycle:transition', 'pp_event', 'ppp_event', 'p_lifecycle:transition']]\n",
    "dfy = df[['next_event']]\n",
    "valy = dfy\n",
    "dummies = pd.get_dummies(dfx)\n",
    "dfy = pd.get_dummies(dfy)\n",
    "dfx = dummies\n",
    "#dfx = dummies.join(df[['day, hour', 'minute', 'month']])\n",
    "trainlen = int(len(df)*0.7)\n",
    "# dfx.iloc[:10]\n",
    "# dfx = dfx.to_numpy()\n",
    "# dfy = dfy.to_numpy()\n",
    "x_train, y_train = dfx[:trainlen], dfy[:trainlen]\n",
    "x_test, y_test = dfx[trainlen:], dfy[trainlen:]\n",
    "dftest2 = x_test.join(y_test)\n",
    "valy = y_test\n",
    "valx = x_test\n",
    "#print(x_train.iloc[:10])\n",
    "columns = y_train.columns\n",
    "x_train = tf.convert_to_tensor(x_train)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "x_test = tf.convert_to_tensor(x_test)\n",
    "y_test = tf.convert_to_tensor(y_test)\n",
    "print(len(valx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Creation of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(x_train.shape[1],1)),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dropout(0.1),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Dense(48, activation='relu'),\n",
    "  tf.keras.layers.Dense(24, activation='softmax')])\n",
    "ftrl = tf.keras.optimizers.Ftrl(\n",
    "    learning_rate=0.1,\n",
    "    learning_rate_power=-0.5,\n",
    "    initial_accumulator_value=0.1,\n",
    "    l1_regularization_strength=0.01,\n",
    "    l2_regularization_strength=0.01,\n",
    "    name=\"Ftrl\",\n",
    "    l2_shrinkage_regularization_strength=0.01,\n",
    "    beta=0.01)\n",
    "nadam = tf.keras.optimizers.Nadam(\n",
    "    learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\")\n",
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=6, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(x_test).numpy()\n",
    "data = tf.nn.softmax(predictions).numpy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=columns)\n",
    "df[:10]\n",
    "predictions = pd.DataFrame(df.idxmax(axis=1), columns = ['next_event_NN_pred'])\n",
    "dftest2['next_event_NN_pred'] = predictions[['next_event_NN_pred']].values\n",
    "dftest2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Combining outputs and exporting CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfffjj = dffj.join(dftest2['next_event_NN_pred'])\n",
    "dfffjj2 = dfffjj\n",
    "dfffjj['next_event_NN_pred'] = dfffjj2['next_event_NN_pred'].str[11:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfffjj3 = dfffjj[(dfffjj['case:concept:name']) ==(dfffjj['next_case'])]\n",
    "dfffjj3.rename(columns={'predictions': 'timedelta_NN_pred'}, inplace=True)\n",
    "dfffjj3.to_csv('outputs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Visualizing the results\n",
    "\n",
    "This section is mainly for producing visualizations for our poster deliverable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('outputs.csv' , index_col=[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourstomin(time):\n",
    "    hours = str(int(time))\n",
    "    minutes = str(int(time*60) % 60).format(\".2f\")\n",
    "    seconds = str(int(time*3600) % 60)\n",
    "    return (hours+'H'+minutes+'m'+seconds+'s')\n",
    "\n",
    "def sectime(time):\n",
    "    time = time/3600\n",
    "    hours = str(int(time))\n",
    "    minutes = str(int(time*60) % 60).format(\".2f\")\n",
    "    seconds = str(int(time*3600) % 60)\n",
    "    return (hours+'H'+minutes+'m'+seconds+'s')\n",
    "\n",
    "\n",
    "def rmse(c1, c2):\n",
    "    err = np.power(np.power((c1-c2), 2).mean(), 0.5)\n",
    "    return err\n",
    "    \n",
    "def MARE(c1, c2):\n",
    "    return np.power(2, abs(np.log2(c1 + 1) - np.log2(c2 + 1.)).mean())\n",
    "\n",
    "def MAE(c1, c2):\n",
    "    return abs(c1 - c2).mean()\n",
    "\n",
    "def r2(c1, c2):\n",
    "    return scipy.stats.pearsonr(c1, c2)\n",
    "def logr2(c1, c2):\n",
    "    return scipy.stats.pearsonr(np.log10(c1+1), np.log10(c2+1))\n",
    "\n",
    "def accuracy(c1, c2, df):\n",
    "    return len(df[c1 == c2])/ len(df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = df['timedelta']\n",
    "tdnn = df['timedelta_NN_pred']\n",
    "tdbl = df['timedelta_baseline']\n",
    "ne = df['next_event']\n",
    "nenn = df['next_event_NN_pred']\n",
    "nerfc = df['next_event_rfc_pred']\n",
    "nebl = df['next_event_baseline']\n",
    "print('mean absolute relative error NN:', MARE(df['timedelta_NN_pred'], df['timedelta']))\n",
    "print('mean absolute relative error baseline: ', MARE(df['timedelta_baseline'], df['timedelta']))\n",
    "print('mean absolute error NN: ', sectime(MAE(df['timedelta_NN_pred'], df['timedelta'])))\n",
    "print('mean absolute error baseline: ',sectime(MAE(tdbl, td)))\n",
    "print('r2 score NN: ', r2(td, tdnn))\n",
    "print('r2 score baseline: ', r2(td, tdbl))\n",
    "print('log r2 score NN: ', logr2(td, tdnn))\n",
    "print('log r2 score baseline: ', logr2(td, tdbl))\n",
    "print('accuracy random forest', accuracy(ne, nerfc, df))\n",
    "print('accuracy NN',accuracy(ne, nenn, df))\n",
    "print('accuracy baseline',accuracy(ne, nebl, df))\n",
    "\n",
    "#sectime(rmse(df['timedelta_baseline'], df['timedelta']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean absolute error\n",
    "\n",
    "print('root mean square eror new:',hourstomin(np.power(np.square(dftest_all['timedelta'] - dftest_all['predictions']).mean(), 0.5)/3600))\n",
    "print('root mean square eror old:',hourstomin(np.power(np.square(dftest_all['timedelta'] - dftest_all['predictions_test']).mean(), 0.5)/3600))\n",
    "print('mean absolute eror new:',hourstomin((abs(dftest_all['timedelta'] - dftest_all['predictions'])).mean()/3600))\n",
    "print('mean absolute eror old:',hourstomin((abs(dftest_all['timedelta'] - dftest_all['predictions_test'])).mean()/3600))\n",
    "print('mean absolute relative error new: ', np.power(2, abs(np.log2(dftest_all['timedelta'] + 1.) - np.log2(dftest_all['predictions'] + 1.)).mean()))\n",
    "print('mean absolute relative error old: ', np.power(2, abs(np.log2(dftest_all['timedelta'] + 1.) - np.log2(dftest_all['predictions_test'] + 1.)).mean()))\n",
    "print('rmse ratio: ', (np.power(np.square(dftest_all['timedelta'] - dftest_all['predictions']).mean(), 0.5) / np.std(dftest_all['timedelta'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = np.log10(dftest_all['predictions']+1).hist(bins=50,  range=[0,6], alpha=0.8, figsize=(12,8), color='red')\n",
    "fig = np.log10(dftest_all['timedelta']+1).hist(bins=50,  range=[0,6], alpha=0.5, color='green')\n",
    "fig = np.log10(tdbl).hist(bins=50,  range=[0,6], alpha=0.5, color='purple')\n",
    "fig.figure.savefig('histograms.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(dftest_all+1).plot.scatter(x='timedelta', y='predictions',alpha=0.1, figsize=(12,12), grid=True, ylim=(-.5,7), xlim=(-.5,7)).figure.savefig('logscatter.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(dftest_all+1).plot.scatter(x='timedelta', y='error',alpha=0.1, figsize=(15,10), grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest_all['timedelta_log'] = np.log10(dftest_all['timedelta']+1)\n",
    "dftest_all['predictions_log'] = np.log10(dftest_all['predictions']+1)\n",
    "sns.set(rc = {'figure.figsize':(8,8)})\n",
    "ax = sns.regplot(x='timedelta_log', y='predictions_log', data=dftest_all, scatter_kws={'alpha':0.01}, fit_reg=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest_all['timedelta_log'] = np.log10(dftest_all['timedelta']+1)\n",
    "dftest_all['predictions_log'] = np.log10(dftest_all['predictions']+1)\n",
    "ax = sns.regplot(x='timedelta_log', y='predictions_log', data=dftest_all, scatter_kws={'alpha':0.01}, fit_reg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr2 = stats.pearsonr(x=dftest_all['timedelta_log'], y=dftest_all['predictions_log'])\n",
    "r2 = stats.pearsonr(x=dftest_all['timedelta'], y=dftest_all['predictions'])\n",
    "print('logr2 = ', logr2)\n",
    "print('r2score = ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end1 = time.time()\n",
    "print(f\"The runtime of the whole tool is {end1 - start1} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
